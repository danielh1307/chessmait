C:\Users\hammd\.virtualenvs\chessmait-3elN5i92\Scripts\python.exe "C:/Program Files/JetBrains/PyCharm 2023.1.1/plugins/python/helpers/pydev/pydevconsole.py" --mode=client --host=127.0.0.1 --port=54487
import sys; print('Python %s on %s' % (sys.version, sys.platform))
sys.path.extend(['C:\\dev\\projects\\chessmait', 'C:\\dev\\projects\\chessmait\\src', 'C:\\dev\\projects\\chessmait\\test'])
PyDev console: starting.
Python 3.11.6 (tags/v3.11.6:8b6ee5b, Oct  2 2023, 14:57:12) [MSC v.1935 64 bit (AMD64)] on win32
runfile('C:\\dev\\projects\\chessmait\\src\\train.py', wdir='C:\\dev\\projects\\chessmait')
Starting training process ...
WANDB_REPORTING for this training is set to True ...
Training on a regression problem ...
For this training, we are going to use cuda device ...
Prepare dataloaders ...
Prepare dataloaders finished ...
wandb: Currently logged in as: hamm-daniel (chessmait). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.12
wandb: Run data is saved locally in C:\dev\projects\chessmait\wandb\run-20231111_200702-uos6efog
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run tough-music-7
wandb:  View project at https://wandb.ai/chessmait/chessmait
wandb:  View run at https://wandb.ai/chessmait/chessmait/runs/uos6efog
Starting training ...
batch 0 from 5630 ...
batch 1000 from 5630 ...
batch 2000 from 5630 ...
batch 3000 from 5630 ...
batch 4000 from 5630 ...
batch 5000 from 5630 ...
{'epoch': 1, 'training loss': 6.925494583701948, 'validation loss': 0.7207051627992769}
batch 0 from 5630 ...
batch 1000 from 5630 ...
batch 2000 from 5630 ...
batch 3000 from 5630 ...
batch 4000 from 5630 ...
batch 5000 from 5630 ...
{'epoch': 2, 'training loss': 3.321527880092617, 'validation loss': 0.6611499928621924}
batch 0 from 5630 ...
batch 1000 from 5630 ...
batch 2000 from 5630 ...
batch 3000 from 5630 ...
batch 4000 from 5630 ...
batch 5000 from 5630 ...
{'epoch': 3, 'training loss': 2.9560138305350847, 'validation loss': 0.6232431671087397}
batch 0 from 5630 ...
batch 1000 from 5630 ...
batch 2000 from 5630 ...
batch 3000 from 5630 ...
batch 4000 from 5630 ...
batch 5000 from 5630 ...
{'epoch': 4, 'training loss': 2.662124020353076, 'validation loss': 0.6093056700919988}
batch 0 from 5630 ...
batch 1000 from 5630 ...
batch 2000 from 5630 ...
batch 3000 from 5630 ...
batch 4000 from 5630 ...
batch 5000 from 5630 ...
{'epoch': 5, 'training loss': 2.4359580524469493, 'validation loss': 0.608348541194573}
batch 0 from 5630 ...
batch 1000 from 5630 ...
batch 2000 from 5630 ...
batch 3000 from 5630 ...
batch 4000 from 5630 ...
batch 5000 from 5630 ...
{'epoch': 6, 'training loss': 2.2496416689100442, 'validation loss': 0.6065960574778728}
batch 0 from 5630 ...
batch 1000 from 5630 ...
batch 2000 from 5630 ...
batch 3000 from 5630 ...
batch 4000 from 5630 ...
batch 5000 from 5630 ...
{'epoch': 7, 'training loss': 2.0955739337550767, 'validation loss': 0.5935613023939368}
batch 0 from 5630 ...
batch 1000 from 5630 ...
batch 2000 from 5630 ...
batch 3000 from 5630 ...
batch 4000 from 5630 ...
batch 5000 from 5630 ...
{'epoch': 8, 'training loss': 1.9711871065264859, 'validation loss': 0.5928733017608465}
batch 0 from 5630 ...
batch 1000 from 5630 ...
batch 2000 from 5630 ...
batch 3000 from 5630 ...
batch 4000 from 5630 ...
batch 5000 from 5630 ...
{'epoch': 9, 'training loss': 1.8857269938816899, 'validation loss': 0.5869896432486712}
batch 0 from 5630 ...
batch 1000 from 5630 ...
batch 2000 from 5630 ...
batch 3000 from 5630 ...
batch 4000 from 5630 ...
batch 5000 from 5630 ...
{'epoch': 10, 'training loss': 1.766514431394171, 'validation loss': 0.5895551010326017}
batch 0 from 5630 ...
batch 1000 from 5630 ...
batch 2000 from 5630 ...
batch 3000 from 5630 ...
batch 4000 from 5630 ...
batch 5000 from 5630 ...
{'epoch': 11, 'training loss': 1.6965105166345893, 'validation loss': 0.5790002963440202}
batch 0 from 5630 ...
batch 1000 from 5630 ...
batch 2000 from 5630 ...
batch 3000 from 5630 ...
batch 4000 from 5630 ...
batch 5000 from 5630 ...
{'epoch': 12, 'training loss': 1.6298993858326867, 'validation loss': 0.6088844970872742}
batch 0 from 5630 ...
batch 1000 from 5630 ...
batch 2000 from 5630 ...
batch 3000 from 5630 ...
batch 4000 from 5630 ...
batch 5000 from 5630 ...
{'epoch': 13, 'training loss': 1.5713231957015523, 'validation loss': 0.5789286015460675}
batch 0 from 5630 ...
batch 1000 from 5630 ...
batch 2000 from 5630 ...
batch 3000 from 5630 ...
batch 4000 from 5630 ...
batch 5000 from 5630 ...
{'epoch': 14, 'training loss': 1.5103987530274026, 'validation loss': 0.5678425422447617}
batch 0 from 5630 ...
batch 1000 from 5630 ...
batch 2000 from 5630 ...
batch 3000 from 5630 ...
batch 4000 from 5630 ...
batch 5000 from 5630 ...
{'epoch': 15, 'training loss': 1.4554863537850906, 'validation loss': 0.6015893803159997}
Training finished ...
wandb: Waiting for W&B process to finish... (success).
wandb:
wandb: Run history:
wandb:           epoch ▁▁▂▃▃▃▄▅▅▅▆▇▇▇█
wandb:   training loss █▃▃▃▂▂▂▂▂▁▁▁▁▁▁
wandb: validation loss █▅▄▃▃▃▂▂▂▂▂▃▂▁▃
wandb:
wandb: Run summary:
wandb:           epoch 15
wandb:   training loss 1.45549
wandb: validation loss 0.60159
wandb:
wandb:  View run tough-music-7 at: https://wandb.ai/chessmait/chessmait/runs/uos6efog
wandb:  View job at https://wandb.ai/chessmait/chessmait/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjExNDgyMTg3Mg==/version_details/v1
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: .\wandb\run-20231111_200702-uos6efog\logs
